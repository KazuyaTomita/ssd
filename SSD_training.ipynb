{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from ssd_v2 import SSD300v2\n",
    "from ssd_512 import SSD512\n",
    "from ssd_training import MultiboxLoss\n",
    "from ssd_utils import BBoxUtility\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "# set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some constants\n",
    "# number of class\n",
    "# Note that it contains background\n",
    "NUM_CLASSES = 2\n",
    "# (300, 300, 3) or (512, 512, 3)\n",
    "# this constant decides used model\n",
    "# if you use SSD300, then you should set (300, 300, 3)\n",
    "input_shape = (300, 300, 3)\n",
    "ssd_300_shape = (300, 300, 3)\n",
    "ssd_512_shape = (512, 512, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "priors = pickle.load(open('prior_boxes_ssd300.pkl'if input_shape == ssd_300_shape else 'prior_boxes_ssd512.pkl', 'rb'))\n",
    "bbox_util = BBoxUtility(NUM_CLASSES, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt = pickle.load(open('kitkat300.pkl'if input_shape == ssd_300_shape else 'kitkat500.pkl', 'rb'))\n",
    "keys = sorted(gt.keys())\n",
    "num_train = int(round(0.8 * len(keys)))\n",
    "train_keys = keys[:num_train]\n",
    "val_keys = keys[num_train:]\n",
    "num_val = len(val_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    def __init__(self, gt, bbox_util,\n",
    "                 batch_size, path_prefix,\n",
    "                 train_keys, val_keys, image_size,\n",
    "                 saturation_var=0.5,\n",
    "                 brightness_var=0.5,\n",
    "                 contrast_var=0.5,\n",
    "                 lighting_std=0.5,\n",
    "                 hflip_prob=0.5,\n",
    "                 vflip_prob=0.5,\n",
    "                 do_crop=True,\n",
    "                 crop_area_range=[0.75, 1.0],\n",
    "                 aspect_ratio_range=[3./4., 4./3.]):\n",
    "        self.gt = gt\n",
    "        self.bbox_util = bbox_util\n",
    "        self.batch_size = batch_size\n",
    "        self.path_prefix = path_prefix\n",
    "        self.train_keys = train_keys\n",
    "        self.val_keys = val_keys\n",
    "        self.train_batches = len(train_keys)\n",
    "        self.val_batches = len(val_keys)\n",
    "        self.image_size = image_size\n",
    "        self.color_jitter = []\n",
    "        if saturation_var:\n",
    "            self.saturation_var = saturation_var\n",
    "            self.color_jitter.append(self.saturation)\n",
    "        if brightness_var:\n",
    "            self.brightness_var = brightness_var\n",
    "            self.color_jitter.append(self.brightness)\n",
    "        if contrast_var:\n",
    "            self.contrast_var = contrast_var\n",
    "            self.color_jitter.append(self.contrast)\n",
    "        self.lighting_std = lighting_std\n",
    "        self.hflip_prob = hflip_prob\n",
    "        self.vflip_prob = vflip_prob\n",
    "        self.do_crop = do_crop\n",
    "        self.crop_area_range = crop_area_range\n",
    "        self.aspect_ratio_range = aspect_ratio_range\n",
    "        \n",
    "    def grayscale(self, rgb):\n",
    "        return rgb.dot([0.299, 0.587, 0.114])\n",
    "\n",
    "    def saturation(self, rgb):\n",
    "        gs = self.grayscale(rgb)\n",
    "        alpha = 2 * np.random.random() * self.saturation_var \n",
    "        alpha += 1 - self.saturation_var\n",
    "        rgb = rgb * alpha + (1 - alpha) * gs[:, :, None]\n",
    "        return np.clip(rgb, 0, 255)\n",
    "\n",
    "    def brightness(self, rgb):\n",
    "        alpha = 2 * np.random.random() * self.brightness_var \n",
    "        alpha += 1 - self.saturation_var\n",
    "        rgb = rgb * alpha\n",
    "        return np.clip(rgb, 0, 255)\n",
    "\n",
    "    def contrast(self, rgb):\n",
    "        gs = self.grayscale(rgb).mean() * np.ones_like(rgb)\n",
    "        alpha = 2 * np.random.random() * self.contrast_var \n",
    "        alpha += 1 - self.contrast_var\n",
    "        rgb = rgb * alpha + (1 - alpha) * gs\n",
    "        return np.clip(rgb, 0, 255)\n",
    "\n",
    "    def lighting(self, img):\n",
    "        cov = np.cov(img.reshape(-1, 3) / 255.0, rowvar=False)\n",
    "        eigval, eigvec = np.linalg.eigh(cov)\n",
    "        noise = np.random.randn(3) * self.lighting_std\n",
    "        noise = eigvec.dot(eigval * noise) * 255\n",
    "        img += noise\n",
    "        return np.clip(img, 0, 255)\n",
    "    \n",
    "    def horizontal_flip(self, img, y):\n",
    "        if np.random.random() < self.hflip_prob:\n",
    "            img = img[:, ::-1]\n",
    "            y[:, [0, 2]] = 1 - y[:, [2, 0]]\n",
    "        return img, y\n",
    "    \n",
    "    def vertical_flip(self, img, y):\n",
    "        if np.random.random() < self.vflip_prob:\n",
    "            img = img[::-1]\n",
    "            y[:, [1, 3]] = 1 - y[:, [3, 1]]\n",
    "        return img, y\n",
    "    \n",
    "    def random_sized_crop(self, img, targets):\n",
    "        img_w = img.shape[1]\n",
    "        img_h = img.shape[0]\n",
    "        img_area = img_w * img_h\n",
    "        random_scale = np.random.random()\n",
    "        random_scale *= (self.crop_area_range[1] -\n",
    "                         self.crop_area_range[0])\n",
    "        random_scale += self.crop_area_range[0]\n",
    "        target_area = random_scale * img_area\n",
    "        random_ratio = np.random.random()\n",
    "        random_ratio *= (self.aspect_ratio_range[1] -\n",
    "                         self.aspect_ratio_range[0])\n",
    "        random_ratio += self.aspect_ratio_range[0]\n",
    "        w = np.round(np.sqrt(target_area * random_ratio))     \n",
    "        h = np.round(np.sqrt(target_area / random_ratio))\n",
    "        if np.random.random() < 0.5:\n",
    "            w, h = h, w\n",
    "        w = min(w, img_w)\n",
    "        w_rel = w / img_w\n",
    "        w = int(w)\n",
    "        h = min(h, img_h)\n",
    "        h_rel = h / img_h\n",
    "        h = int(h)\n",
    "        x = np.random.random() * (img_w - w)\n",
    "        x_rel = x / img_w\n",
    "        x = int(x)\n",
    "        y = np.random.random() * (img_h - h)\n",
    "        y_rel = y / img_h\n",
    "        y = int(y)\n",
    "        img = img[y:y+h, x:x+w]\n",
    "        new_targets = []\n",
    "        for box in targets:\n",
    "            cx = 0.5 * (box[0] + box[2])\n",
    "            cy = 0.5 * (box[1] + box[3])\n",
    "            if (x_rel < cx < x_rel + w_rel and\n",
    "                y_rel < cy < y_rel + h_rel):\n",
    "                xmin = (box[0] - x_rel) / w_rel\n",
    "                ymin = (box[1] - y_rel) / h_rel\n",
    "                xmax = (box[2] - x_rel) / w_rel\n",
    "                ymax = (box[3] - y_rel) / h_rel\n",
    "                xmin = max(0, xmin)\n",
    "                ymin = max(0, ymin)\n",
    "                xmax = min(1, xmax)\n",
    "                ymax = min(1, ymax)\n",
    "                box[:4] = [xmin, ymin, xmax, ymax]\n",
    "                new_targets.append(box)\n",
    "        new_targets = np.asarray(new_targets).reshape(-1, targets.shape[1])\n",
    "        return img, new_targets\n",
    "    \n",
    "    def generate(self, train=True):\n",
    "        while True:\n",
    "            if train:\n",
    "                shuffle(self.train_keys)\n",
    "                keys = self.train_keys\n",
    "            else:\n",
    "                shuffle(self.val_keys)\n",
    "                keys = self.val_keys\n",
    "            inputs = []\n",
    "            targets = []\n",
    "            for key in keys:            \n",
    "                img_path = self.path_prefix + key\n",
    "                img = imread(img_path).astype('float32')\n",
    "                y = self.gt[key].copy()\n",
    "                if train and self.do_crop:\n",
    "                    img, y = self.random_sized_crop(img, y)\n",
    "                img = imresize(img, self.image_size).astype('float32')\n",
    "                if train:\n",
    "                    shuffle(self.color_jitter)\n",
    "                    for jitter in self.color_jitter:\n",
    "                        img = jitter(img)\n",
    "                    if self.lighting_std:\n",
    "                        img = self.lighting(img)\n",
    "                    if self.hflip_prob > 0:\n",
    "                        img, y = self.horizontal_flip(img, y)\n",
    "                    if self.vflip_prob > 0:\n",
    "                        img, y = self.vertical_flip(img, y)\n",
    "                y = self.bbox_util.assign_boxes(y)\n",
    "                inputs.append(img)                \n",
    "                targets.append(y)\n",
    "                if len(targets) == self.batch_size:\n",
    "                    tmp_inp = np.array(inputs)\n",
    "                    tmp_targets = np.array(targets)\n",
    "                    inputs = []\n",
    "                    targets = []\n",
    "                    yield preprocess_input(tmp_inp), tmp_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_prefix = './training_set300/JPEGImages/' if input_shape == ssd_300_shape else './training_set500/JPEGImages/'\n",
    "gen = Generator(gt, bbox_util, 4, path_prefix,\n",
    "                train_keys, val_keys,\n",
    "                (input_shape[0], input_shape[1]), do_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/ssd_keras-master/ssd.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv1_1\", padding=\"same\")`\n",
      "  name='conv1_1')(net['input'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv1_2\", padding=\"same\")`\n",
      "  name='conv1_2')(net['conv1_1'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:46: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), strides=(2, 2), name=\"pool1\", padding=\"same\")`\n",
      "  name='pool1')(net['conv1_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv2_1\", padding=\"same\")`\n",
      "  name='conv2_1')(net['pool1'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv2_2\", padding=\"same\")`\n",
      "  name='conv2_2')(net['conv2_1'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:57: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), strides=(2, 2), name=\"pool2\", padding=\"same\")`\n",
      "  name='pool2')(net['conv2_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_1\", padding=\"same\")`\n",
      "  name='conv3_1')(net['pool2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:66: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_2\", padding=\"same\")`\n",
      "  name='conv3_2')(net['conv3_1'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:70: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv3_3\", padding=\"same\")`\n",
      "  name='conv3_3')(net['conv3_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:72: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), strides=(2, 2), name=\"pool3\", padding=\"same\")`\n",
      "  name='pool3')(net['conv3_3'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_1\", padding=\"same\")`\n",
      "  name='conv4_1')(net['pool3'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_2\", padding=\"same\")`\n",
      "  name='conv4_2')(net['conv4_1'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:85: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv4_3\", padding=\"same\")`\n",
      "  name='conv4_3')(net['conv4_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:87: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((2, 2), strides=(2, 2), name=\"pool4\", padding=\"same\")`\n",
      "  name='pool4')(net['conv4_3'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:92: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_1\", padding=\"same\")`\n",
      "  name='conv5_1')(net['pool4'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:96: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_2\", padding=\"same\")`\n",
      "  name='conv5_2')(net['conv5_1'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:100: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv5_3\", padding=\"same\")`\n",
      "  name='conv5_3')(net['conv5_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:102: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((3, 3), strides=(1, 1), name=\"pool5\", padding=\"same\")`\n",
      "  name='pool5')(net['conv5_3'])\n",
      "/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/layers.py:757: UserWarning: The `AtrousConvolution2D` layer  has been deprecated. Use instead the `Conv2D` layer with the `dilation_rate` argument.\n",
      "  warnings.warn('The `AtrousConvolution2D` layer '\n",
      "/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/layers.py:761: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (3, 3), activation=\"relu\", name=\"fc6\", dilation_rate=(6, 6), padding=\"same\")`\n",
      "  return Conv2D(*args, **kwargs)\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:110: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (1, 1), activation=\"relu\", name=\"fc7\", padding=\"same\")`\n",
      "  border_mode='same', name='fc7')(net['fc6'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:115: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), activation=\"relu\", name=\"conv6_1\", padding=\"same\")`\n",
      "  name='conv6_1')(net['fc7'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:118: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\", name=\"conv6_2\", strides=(2, 2), padding=\"same\")`\n",
      "  name='conv6_2')(net['conv6_1'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:122: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"conv7_1\", padding=\"same\")`\n",
      "  name='conv7_1')(net['conv6_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv7_2\", strides=(2, 2), padding=\"valid\")`\n",
      "  name='conv7_2')(net['conv7_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:130: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"conv8_1\", padding=\"same\")`\n",
      "  name='conv8_1')(net['conv7_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:133: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv8_2\", strides=(2, 2), padding=\"same\")`\n",
      "  name='conv8_2')(net['conv8_1'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:140: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), name=\"conv4_3_norm_mbox_loc\", padding=\"same\")`\n",
      "  name='conv4_3_norm_mbox_loc')(net['conv4_3_norm'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:148: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (3, 3), name=\"conv4_3_norm_mbox_conf_1\", padding=\"same\")`\n",
      "  name=name)(net['conv4_3_norm'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:160: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), name=\"fc7_mbox_loc\", padding=\"same\")`\n",
      "  name='fc7_mbox_loc')(net['fc7'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:168: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(6, (3, 3), name=\"fc7_mbox_conf_1\", padding=\"same\")`\n",
      "  name=name)(net['fc7'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:178: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), name=\"conv6_2_mbox_loc\", padding=\"same\")`\n",
      "  name='conv6_2_mbox_loc')(net['conv6_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:186: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(6, (3, 3), name=\"conv6_2_mbox_conf_1\", padding=\"same\")`\n",
      "  name=name)(net['conv6_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:197: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), name=\"conv7_2_mbox_loc\", padding=\"same\")`\n",
      "  name='conv7_2_mbox_loc')(net['conv7_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:205: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(6, (3, 3), name=\"conv7_2_mbox_conf_1\", padding=\"same\")`\n",
      "  name=name)(net['conv7_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:216: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), name=\"conv8_2_mbox_loc\", padding=\"same\")`\n",
      "  name='conv8_2_mbox_loc')(net['conv8_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:224: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(6, (3, 3), name=\"conv8_2_mbox_conf_1\", padding=\"same\")`\n",
      "  name=name)(net['conv8_2'])\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:258: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  mode='concat', concat_axis=1, name='mbox_loc')\n",
      "/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:265: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  mode='concat', concat_axis=1, name='mbox_conf')\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:273: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name='mbox_priorbox')\n",
      "/home/ubuntu/src/ssd_keras-master/ssd.py:288: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name='predictions')\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "model = SSD300v2(input_shape, num_classes=NUM_CLASSES) if input_shape == ssd_300_shape else SSD512(input_shape, num_classes=NUM_CLASSES)\n",
    "model.load_weights('weights_SSD300.hdf5' if input_shape == ssd_300_shape else 'VGG_VOC0712Plus_SSD_512x512_ft_iter_160000.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freeze = ['input_1', 'conv1_1', 'conv1_2', 'pool1',\n",
    "          'conv2_1', 'conv2_2', 'pool2',\n",
    "          'conv3_1', 'conv3_2', 'conv3_3', 'pool3']#,\n",
    "#           'conv4_1', 'conv4_2', 'conv4_3', 'pool4']\n",
    "\n",
    "for L in model.layers:\n",
    "    if L.name in freeze:\n",
    "        L.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(epoch, decay=0.9):\n",
    "    return base_lr * decay**(epoch)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint('./checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                             verbose=1,\n",
    "                                             save_weights_only=True),\n",
    "             keras.callbacks.LearningRateScheduler(schedule), early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_lr = 3e-4\n",
    "optim = keras.optimizers.Adam(lr=base_lr)\n",
    "# optim = keras.optimizers.RMSprop(lr=base_lr)\n",
    "# optim = keras.optimizers.SGD(lr=base_lr, momentum=0.9, decay=decay, nesterov=True)\n",
    "\n",
    "# def calculate_mAP(y_true,y_pred):\n",
    "#     num_classes = y_true.shape[1]\n",
    "#     average_precisions = []\n",
    "#     relevant = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "#     tp_whole = K.round(K.clip(y_true * y_pred, 0, 1))\n",
    "#     for index in range(num_classes):\n",
    "#         temp = K.sum(tp_whole[:,:index+1],axis=1)\n",
    "#         average_precisions.append(temp * (1/(index + 1)))\n",
    "#     AP = Add()(average_precisions) / relevant\n",
    "#     mAP = K.mean(AP,axis=0)\n",
    "#     return mAP\n",
    "\n",
    "\n",
    "model.compile(optimizer=optim,\n",
    "              loss=MultiboxLoss(NUM_CLASSES, neg_pos_ratio=2.0).compute_loss,\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tictoc = strftime(\"%a_%d_%b_%Y_%H_%M_%S\", gmtime())                             \n",
    "directory_name = tictoc\n",
    "log_dir = directory_name\n",
    "os.mkdir(log_dir)\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True,)\n",
    "callbacks.append(tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., 120, 1, verbose=1, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=30, workers=1)`\n",
      "  import sys\n",
      "/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv1_1/kernel:0 is illegal; using conv1_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1/bias:0 is illegal; using conv1_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_2/kernel:0 is illegal; using conv1_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_2/bias:0 is illegal; using conv1_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv2_1/kernel:0 is illegal; using conv2_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv2_1/bias:0 is illegal; using conv2_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv2_2/kernel:0 is illegal; using conv2_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv2_2/bias:0 is illegal; using conv2_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv3_1/kernel:0 is illegal; using conv3_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv3_1/bias:0 is illegal; using conv3_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv3_2/kernel:0 is illegal; using conv3_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv3_2/bias:0 is illegal; using conv3_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv3_3/kernel:0 is illegal; using conv3_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv3_3/bias:0 is illegal; using conv3_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_1/kernel:0 is illegal; using conv4_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_1/bias:0 is illegal; using conv4_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_2/kernel:0 is illegal; using conv4_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_2/bias:0 is illegal; using conv4_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_3/kernel:0 is illegal; using conv4_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_3/bias:0 is illegal; using conv4_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv5_1/kernel:0 is illegal; using conv5_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv5_1/bias:0 is illegal; using conv5_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv5_2/kernel:0 is illegal; using conv5_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv5_2/bias:0 is illegal; using conv5_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv5_3/kernel:0 is illegal; using conv5_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv5_3/bias:0 is illegal; using conv5_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name fc6/kernel:0 is illegal; using fc6/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name fc6/bias:0 is illegal; using fc6/bias_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/kernel:0 is illegal; using fc7/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/bias:0 is illegal; using fc7/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv6_1/kernel:0 is illegal; using conv6_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv6_1/bias:0 is illegal; using conv6_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv6_2/kernel:0 is illegal; using conv6_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv6_2/bias:0 is illegal; using conv6_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv7_1/kernel:0 is illegal; using conv7_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv7_1/bias:0 is illegal; using conv7_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv7_2/kernel:0 is illegal; using conv7_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv7_2/bias:0 is illegal; using conv7_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv8_1/kernel:0 is illegal; using conv8_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv8_1/bias:0 is illegal; using conv8_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_3_norm/conv4_3_norm_gamma:0 is illegal; using conv4_3_norm/conv4_3_norm_gamma_0 instead.\n",
      "INFO:tensorflow:Summary name conv8_2/kernel:0 is illegal; using conv8_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv8_2/bias:0 is illegal; using conv8_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_3_norm_mbox_conf_1/kernel:0 is illegal; using conv4_3_norm_mbox_conf_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_3_norm_mbox_conf_1/bias:0 is illegal; using conv4_3_norm_mbox_conf_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name fc7_mbox_conf_1/kernel:0 is illegal; using fc7_mbox_conf_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name fc7_mbox_conf_1/bias:0 is illegal; using fc7_mbox_conf_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv6_2_mbox_conf_1/kernel:0 is illegal; using conv6_2_mbox_conf_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv6_2_mbox_conf_1/bias:0 is illegal; using conv6_2_mbox_conf_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv7_2_mbox_conf_1/kernel:0 is illegal; using conv7_2_mbox_conf_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv7_2_mbox_conf_1/bias:0 is illegal; using conv7_2_mbox_conf_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv8_2_mbox_conf_1/kernel:0 is illegal; using conv8_2_mbox_conf_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv8_2_mbox_conf_1/bias:0 is illegal; using conv8_2_mbox_conf_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_3_norm_mbox_loc/kernel:0 is illegal; using conv4_3_norm_mbox_loc/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv4_3_norm_mbox_loc/bias:0 is illegal; using conv4_3_norm_mbox_loc/bias_0 instead.\n",
      "INFO:tensorflow:Summary name fc7_mbox_loc/kernel:0 is illegal; using fc7_mbox_loc/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name fc7_mbox_loc/bias:0 is illegal; using fc7_mbox_loc/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv6_2_mbox_loc/kernel:0 is illegal; using conv6_2_mbox_loc/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv6_2_mbox_loc/bias:0 is illegal; using conv6_2_mbox_loc/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv7_2_mbox_loc/kernel:0 is illegal; using conv7_2_mbox_loc/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv7_2_mbox_loc/bias:0 is illegal; using conv7_2_mbox_loc/bias_0 instead.\n",
      "INFO:tensorflow:Summary name conv8_2_mbox_loc/kernel:0 is illegal; using conv8_2_mbox_loc/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name conv8_2_mbox_loc/bias:0 is illegal; using conv8_2_mbox_loc/bias_0 instead.\n",
      "INFO:tensorflow:Summary name pool6_mbox_conf_flat_1/kernel:0 is illegal; using pool6_mbox_conf_flat_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name pool6_mbox_conf_flat_1/bias:0 is illegal; using pool6_mbox_conf_flat_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name pool6_mbox_loc_flat/kernel:0 is illegal; using pool6_mbox_loc_flat/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name pool6_mbox_loc_flat/bias:0 is illegal; using pool6_mbox_loc_flat/bias_0 instead.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/ssd_keras-master/ssd_utils.py:114: RuntimeWarning: divide by zero encountered in log\n",
      "  assigned_priors_wh)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/120 [============================>.] - ETA: 14s - loss: 4.9876Epoch 00000: saving model to ./checkpoints/weights.00-1.20.hdf5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (Unable to open file: name = './checkpoints/weights.00-1.20.hdf5', errno = 2, error message = 'no such file or directory', flags = 13, o_flags = 242)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1b31dc2db8e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                               \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                               nb_worker=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1937\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1939\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1940\u001b[0m                 \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1941\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %05d: saving model to %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m   2534\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproceed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2535\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2536\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2537\u001b[0m         \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1490028290543/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1490028290543/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create (/home/ilan/minonda/conda-bld/h5py_1490028290543/work/h5py/h5f.c:2290)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (Unable to open file: name = './checkpoints/weights.00-1.20.hdf5', errno = 2, error message = 'no such file or directory', flags = 13, o_flags = 242)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "nb_epoch = 30\n",
    "history = model.fit_generator(gen.generate(True), gen.train_batches,\n",
    "                              nb_epoch, verbose=1,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=gen.generate(False),\n",
    "                              nb_val_samples=gen.val_batches,\n",
    "                              nb_worker=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}